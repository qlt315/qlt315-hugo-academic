---
title: "Inferring Motion Direction using Commodity Wi-Fi for Interactive Exergames"

authors:
- Kun Qian
- Chenshu Wu
- Zimu Zhou
- Yue Zheng
- Zheng Yang
- Yunhao Liu

date: 2017-05-06T00:00:00.000Z
doi: ""

publication_types:
  - "1"

publication: In Proceedings of the CHI conference on human factors in computing systems (**CHI**) 2017
publication_short: In Proceedings of the CHI conference on human factors in computing systems (**CHI**)

abstract: 'In-air interaction acts as a key enabler for ambient intelligence and augmented reality. As an increasing popular example, exergames, and the alike gesture recognition applications, have attracted extensive research in designing accurate, pervasive and low-cost user interfaces. Recent advances in wireless sensing show promise for a ubiquitous gesture-based interaction interface with Wi-Fi. In this work, we extract complete information of motion-induced Doppler shifts with only commodity Wi-Fi. The key insight is to harness antenna diversity to carefully eliminate random phase shifts while retaining relevant Doppler shifts. We further correlate Doppler shifts with motion directions, and propose a light-weight pipeline to detect, segment, and recognize motions without training. On this basis, we present WiDance, a Wi-Fi-based user interface, which we utilize to design and prototype a contactless dance-pad exergame. Experimental results in typical indoor environment demonstrate a superior performance with an accuracy of 92%, remarkably outperforming prior approaches.'

tags:

draft: false
featured: false

url_pdf: 'https://www.dropbox.com/s/8vzkhh7fk3d0jlx/CHI17_WiDance_paper.pdf?dl=0'
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: 'https://www.dropbox.com/s/bg9o6213rf43z1t/CHI17_WiDance_slides.pptx?dl=0'
url_source: ''
url_video: 'https://www.youtube.com/watch?v=-Snenj43hKA'

image:
  caption: "The HCI interface of WiDance."
  filename: featured
  focal_point: Smart
  preview_only: false
  
projects:
---
<span style="color:red"><strong>Honorable Mention Award, top 5% submission</strong></span>
